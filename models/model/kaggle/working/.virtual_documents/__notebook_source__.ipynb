import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import pickle
import warnings
warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")


DIABETES_CSV = '/kaggle/input/pima-indians-diabetes-database/diabetes.csv'
HEART_CSV = '/kaggle/input/heart-disease-dataset/heart.csv'
RETINAL_BASE = '/kaggle/input/diabetic-retinopathy-224x224-2019-data/colored_images'
ECG_BASE = '/kaggle/input/ecg-analysis/ecg_data_new_version/ecg data new version'
RETINAL_LABELS = '/kaggle/input/diabetic-retinopathy-224x224-2019-data/train.csv'

IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20
SEED = 42

np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)

print("Configuration loaded successfully")
print(f"Image size: {IMG_SIZE}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Epochs: {EPOCHS}")
print(f"Random seed: {SEED}")


df_diabetes = pd.read_csv(DIABETES_CSV)
print("DIABETES CSV DATASET")
print("="*50)
print(f"Shape: {df_diabetes.shape}")
print(f"\nFirst 3 rows:\n{df_diabetes.head(3)}")
print(f"\nMissing values: {df_diabetes.isnull().sum().sum()}")
print(f"\nTarget distribution:\n{df_diabetes['Outcome'].value_counts()}")
print(f"\nClass balance: {df_diabetes['Outcome'].value_counts(normalize=True)}")


X_diabetes = df_diabetes.drop('Outcome', axis=1)
y_diabetes = df_diabetes['Outcome']

X_temp, X_test_diabetes, y_temp, y_test_diabetes = train_test_split(
    X_diabetes, y_diabetes, test_size=0.2, random_state=SEED, stratify=y_diabetes
)

X_train_diabetes, X_val_diabetes, y_train_diabetes, y_val_diabetes = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=SEED, stratify=y_temp
)

scaler_diabetes = StandardScaler()
X_train_diabetes_scaled = scaler_diabetes.fit_transform(X_train_diabetes)
X_val_diabetes_scaled = scaler_diabetes.transform(X_val_diabetes)
X_test_diabetes_scaled = scaler_diabetes.transform(X_test_diabetes)

print("DIABETES DATA SPLIT (60/20/20)")
print("="*50)
print(f"Train set: {X_train_diabetes_scaled.shape}")
print(f"Val set: {X_val_diabetes_scaled.shape}")
print(f"Test set: {X_test_diabetes_scaled.shape}")
print(f"\nTrain distribution:\n{pd.Series(y_train_diabetes).value_counts()}")
print(f"\nVal distribution:\n{pd.Series(y_val_diabetes).value_counts()}")
print(f"\nTest distribution:\n{pd.Series(y_test_diabetes).value_counts()}")


xgb_diabetes = xgb.XGBClassifier(
    n_estimators=30,
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=SEED,
    eval_metric='logloss'
)

xgb_diabetes.fit(
    X_train_diabetes_scaled, y_train_diabetes,
    eval_set=[(X_val_diabetes_scaled, y_val_diabetes)],
    verbose=False
)

y_train_pred = xgb_diabetes.predict(X_train_diabetes_scaled)
y_val_pred = xgb_diabetes.predict(X_val_diabetes_scaled)
y_test_pred = xgb_diabetes.predict(X_test_diabetes_scaled)

train_acc = accuracy_score(y_train_diabetes, y_train_pred)
val_acc = accuracy_score(y_val_diabetes, y_val_pred)
test_acc = accuracy_score(y_test_diabetes, y_test_pred)

print("DIABETES XGBOOST MODEL")
print("="*50)
print(f"Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)")
print(f"Val Accuracy:   {val_acc:.4f} ({val_acc*100:.2f}%)")
print(f"Test Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"\nOverfit gap (Train-Test): {(train_acc-test_acc)*100:.2f}%")
print(f"\nTest Classification Report:\n{classification_report(y_test_diabetes, y_test_pred)}")


with open('diabetes_xgboost_model.pkl', 'wb') as f:
    pickle.dump(xgb_diabetes, f)
with open('diabetes_scaler.pkl', 'wb') as f:
    pickle.dump(scaler_diabetes, f)

cm = confusion_matrix(y_test_diabetes, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Diabetes XGBoost - Test Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

print("Diabetes XGBoost model and scaler saved successfully")


df_heart = pd.read_csv(HEART_CSV)
print("HEART DISEASE CSV DATASET")
print("="*50)
print(f"Shape: {df_heart.shape}")
print(f"\nFirst 3 rows:\n{df_heart.head(3)}")
print(f"\nMissing values: {df_heart.isnull().sum().sum()}")
print(f"\nTarget distribution:\n{df_heart['target'].value_counts()}")
print(f"\nClass balance: {df_heart['target'].value_counts(normalize=True)}")


X_heart = df_heart.drop('target', axis=1)
y_heart = df_heart['target']

X_temp, X_test_heart, y_temp, y_test_heart = train_test_split(
    X_heart, y_heart, test_size=0.2, random_state=SEED, stratify=y_heart
)

X_train_heart, X_val_heart, y_train_heart, y_val_heart = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=SEED, stratify=y_temp
)

scaler_heart = StandardScaler()
X_train_heart_scaled = scaler_heart.fit_transform(X_train_heart)
X_val_heart_scaled = scaler_heart.transform(X_val_heart)
X_test_heart_scaled = scaler_heart.transform(X_test_heart)

print("HEART DISEASE DATA SPLIT (60/20/20)")
print("="*50)
print(f"Train set: {X_train_heart_scaled.shape}")
print(f"Val set: {X_val_heart_scaled.shape}")
print(f"Test set: {X_test_heart_scaled.shape}")
print(f"\nTrain distribution:\n{pd.Series(y_train_heart).value_counts()}")
print(f"\nVal distribution:\n{pd.Series(y_val_heart).value_counts()}")
print(f"\nTest distribution:\n{pd.Series(y_test_heart).value_counts()}")


xgb_heart = xgb.XGBClassifier(
    n_estimators=30,
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=SEED,
    eval_metric='logloss'
)

xgb_heart.fit(
    X_train_heart_scaled, y_train_heart,
    eval_set=[(X_val_heart_scaled, y_val_heart)],
    verbose=False
)

y_train_pred = xgb_heart.predict(X_train_heart_scaled)
y_val_pred = xgb_heart.predict(X_val_heart_scaled)
y_test_pred = xgb_heart.predict(X_test_heart_scaled)

train_acc = accuracy_score(y_train_heart, y_train_pred)
val_acc = accuracy_score(y_val_heart, y_val_pred)
test_acc = accuracy_score(y_test_heart, y_test_pred)

print("HEART DISEASE XGBOOST MODEL")
print("="*50)
print(f"Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)")
print(f"Val Accuracy:   {val_acc:.4f} ({val_acc*100:.2f}%)")
print(f"Test Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"\nOverfit gap (Train-Test): {(train_acc-test_acc)*100:.2f}%")
print(f"\nTest Classification Report:\n{classification_report(y_test_heart, y_test_pred)}")


with open('heart_xgboost_model.pkl', 'wb') as f:
    pickle.dump(xgb_heart, f)
with open('heart_scaler.pkl', 'wb') as f:
    pickle.dump(scaler_heart, f)

cm = confusion_matrix(y_test_heart, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title('Heart Disease XGBoost - Test Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

print("Heart XGBoost model and scaler saved successfully")


retinal_labels_df = pd.read_csv(RETINAL_LABELS)
print("RETINAL IMAGE DATASET")
print("="*50)
print(f"Labels CSV shape: {retinal_labels_df.shape}")
print(f"\nFirst 3 rows:\n{retinal_labels_df.head(3)}")
print(f"\nOriginal class distribution:\n{retinal_labels_df['diagnosis'].value_counts().sort_index()}")

retinal_labels_df['binary_label'] = (retinal_labels_df['diagnosis'] > 0).astype(int)
print(f"\nBinary class distribution:\n{retinal_labels_df['binary_label'].value_counts()}")

class_folders = {
    0: 'No_DR',
    1: 'Mild',
    2: 'Moderate',
    3: 'Severe',
    4: 'Proliferate_DR'
}

image_paths = []
labels = []

for idx, row in retinal_labels_df.iterrows():
    img_id = row['id_code']
    label = row['binary_label']
    orig_label = row['diagnosis']
    folder_name = class_folders[orig_label]
    img_path = f"{RETINAL_BASE}/{folder_name}/{img_id}.png"
    if os.path.exists(img_path):
        image_paths.append(img_path)
        labels.append(label)

print(f"\nTotal images found: {len(image_paths)}")
print(f"Binary label distribution:\n{pd.Series(labels).value_counts()}")


retinal_df = pd.DataFrame({'image_path': image_paths, 'label': labels})

temp_df, test_retinal_df = train_test_split(
    retinal_df, test_size=0.2, random_state=SEED, stratify=retinal_df['label']
)

train_retinal_df, val_retinal_df = train_test_split(
    temp_df, test_size=0.25, random_state=SEED, stratify=temp_df['label']
)

print("RETINAL IMAGE DATA SPLIT (60/20/20)")
print("="*50)
print(f"Train images: {len(train_retinal_df)}")
print(f"Val images: {len(val_retinal_df)}")
print(f"Test images: {len(test_retinal_df)}")
print(f"\nTrain distribution:\n{train_retinal_df['label'].value_counts()}")
print(f"\nVal distribution:\n{val_retinal_df['label'].value_counts()}")
print(f"\nTest distribution:\n{test_retinal_df['label'].value_counts()}")


class RetinalDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe.reset_index(drop=True)
        self.transform = transform
    
    def __len__(self):
        return len(self.dataframe)
    
    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx]['image_path']
        label = self.dataframe.iloc[idx]['label']
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, label

train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.3),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_dataset = RetinalDataset(train_retinal_df, transform=train_transform)
val_dataset = RetinalDataset(val_retinal_df, transform=test_transform)
test_dataset = RetinalDataset(test_retinal_df, transform=test_transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print("RETINAL IMAGE DATALOADERS CREATED")
print("="*50)
print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")
print(f"Test batches: {len(test_loader)}")


model_retinal = models.efficientnet_b0(weights='IMAGENET1K_V1')

for param in model_retinal.features[:-3].parameters():
    param.requires_grad = False

num_features = model_retinal.classifier[1].in_features
model_retinal.classifier = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, 128),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(128, 2)
)

model_retinal = model_retinal.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_retinal.parameters(), lr=0.00005, weight_decay=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

print("RETINAL MODEL ARCHITECTURE")
print("="*50)
print(f"Model: EfficientNet-B0")
print(f"Frozen layers: features[:-3]")
print(f"Dropout: 0.5 and 0.4")
print(f"Learning rate: 0.00005")
print(f"Weight decay: 0.001")
print(f"\nClassifier:\n{model_retinal.classifier}")


print("TRAINING RETINAL MODEL")
print("="*50)

train_losses = []
train_accs = []
val_losses = []
val_accs = []
best_val_acc = 0.0

RETINAL_EPOCHS = 2

for epoch in range(RETINAL_EPOCHS):
    model_retinal.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model_retinal(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    train_loss = running_loss / len(train_loader)
    train_acc = 100. * correct / total
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    
    model_retinal.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model_retinal(images)
            loss = criterion(outputs, labels)
            
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    val_loss = val_loss / len(val_loader)
    val_acc = 100. * correct / total
    val_losses.append(val_loss)
    val_accs.append(val_acc)
    
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model_retinal.state_dict(), 'diabetes_retinal_model.pth')
    
    print(f'Epoch [{epoch+1}/{RETINAL_EPOCHS}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}%')

print(f"\nBest Validation Accuracy: {best_val_acc:.2f}%")


model_retinal.load_state_dict(torch.load('diabetes_retinal_model.pth'))
model_retinal.eval()

all_preds = []
all_labels = []
test_loss = 0.0

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model_retinal(images)
        loss = criterion(outputs, labels)
        test_loss += loss.item()
        
        _, predicted = outputs.max(1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

test_loss = test_loss / len(test_loader)
test_acc = accuracy_score(all_labels, all_preds)

print("RETINAL MODEL FINAL RESULTS")
print("="*50)
print(f"Train Accuracy: {train_accs[-1]:.2f}%")
print(f"Val Accuracy:   {val_accs[-1]:.2f}%")
print(f"Test Accuracy:  {test_acc*100:.2f}%")
print(f"\nOverfit gap (Train-Test): {(train_accs[-1]-test_acc*100):.2f}%")
print(f"\nTest Classification Report:\n{classification_report(all_labels, all_preds, target_names=['No DR', 'Has DR'])}")

cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Retinal Model - Test Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

print("\nRetinal model saved as diabetes_retinal_model.pth")


ecg_image_paths = []
ecg_labels = []

ecg_class_mapping = {
    'normal_ecg_images': 0,
    'abnormal_heartbeat_ecg_images': 1,
    'myocardial_infarction_ecg_images': 1,
    'post_mi_history_ecg_images': 1
}

for class_folder, label in ecg_class_mapping.items():
    folder_path = os.path.join(ECG_BASE, class_folder)
    if os.path.exists(folder_path):
        for img_file in os.listdir(folder_path):
            if img_file.endswith(('.png', '.jpg', '.jpeg')):
                img_path = os.path.join(folder_path, img_file)
                ecg_image_paths.append(img_path)
                ecg_labels.append(label)

print("ECG IMAGE DATASET")
print("="*50)
print(f"Total images found: {len(ecg_image_paths)}")
print(f"\nBinary label distribution:\n{pd.Series(ecg_labels).value_counts()}")
print(f"\nClass balance: {pd.Series(ecg_labels).value_counts(normalize=True)}")


ecg_df = pd.DataFrame({'image_path': ecg_image_paths, 'label': ecg_labels})

temp_df, test_ecg_df = train_test_split(
    ecg_df, test_size=0.2, random_state=SEED, stratify=ecg_df['label']
)

train_ecg_df, val_ecg_df = train_test_split(
    temp_df, test_size=0.25, random_state=SEED, stratify=temp_df['label']
)

print("ECG IMAGE DATA SPLIT (60/20/20)")
print("="*50)
print(f"Train images: {len(train_ecg_df)}")
print(f"Val images: {len(val_ecg_df)}")
print(f"Test images: {len(test_ecg_df)}")
print(f"\nTrain distribution:\n{train_ecg_df['label'].value_counts()}")
print(f"\nVal distribution:\n{val_ecg_df['label'].value_counts()}")
print(f"\nTest distribution:\n{test_ecg_df['label'].value_counts()}")


class ECGDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe.reset_index(drop=True)
        self.transform = transform
    
    def __len__(self):
        return len(self.dataframe)
    
    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx]['image_path']
        label = self.dataframe.iloc[idx]['label']
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, label

train_ecg_dataset = ECGDataset(train_ecg_df, transform=train_transform)
val_ecg_dataset = ECGDataset(val_ecg_df, transform=test_transform)
test_ecg_dataset = ECGDataset(test_ecg_df, transform=test_transform)

train_ecg_loader = DataLoader(train_ecg_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_ecg_loader = DataLoader(val_ecg_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_ecg_loader = DataLoader(test_ecg_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print("ECG IMAGE DATALOADERS CREATED")
print("="*50)
print(f"Train batches: {len(train_ecg_loader)}")
print(f"Val batches: {len(val_ecg_loader)}")
print(f"Test batches: {len(test_ecg_loader)}")


model_ecg = models.resnet50(weights='IMAGENET1K_V1')

for param in model_ecg.layer1.parameters():
    param.requires_grad = False
for param in model_ecg.layer2.parameters():
    param.requires_grad = False

num_features = model_ecg.fc.in_features
model_ecg.fc = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, 128),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(128, 2)
)

model_ecg = model_ecg.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_ecg.parameters(), lr=0.00005, weight_decay=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

print("ECG MODEL ARCHITECTURE")
print("="*50)
print(f"Model: ResNet50")
print(f"Frozen layers: layer1, layer2")
print(f"Dropout: 0.5 and 0.4")
print(f"Learning rate: 0.00005")
print(f"Weight decay: 0.001")
print(f"\nClassifier:\n{model_ecg.fc}")


print("TRAINING ECG MODEL")
print("="*50)

train_losses_ecg = []
train_accs_ecg = []
val_losses_ecg = []
val_accs_ecg = []
best_val_acc_ecg = 0.0

ECG_EPOCHS = 3

for epoch in range(ECG_EPOCHS):
    model_ecg.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for images, labels in train_ecg_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model_ecg(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    train_loss = running_loss / len(train_ecg_loader)
    train_acc = 100. * correct / total
    train_losses_ecg.append(train_loss)
    train_accs_ecg.append(train_acc)
    
    model_ecg.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_ecg_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model_ecg(images)
            loss = criterion(outputs, labels)
            
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    val_loss = val_loss / len(val_ecg_loader)
    val_acc = 100. * correct / total
    val_losses_ecg.append(val_loss)
    val_accs_ecg.append(val_acc)
    
    if val_acc > best_val_acc_ecg:
        best_val_acc_ecg = val_acc
        torch.save(model_ecg.state_dict(), 'heart_ecg_model.pth')
    
    print(f'Epoch [{epoch+1}/{ECG_EPOCHS}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}%')

print(f"\nBest Validation Accuracy: {best_val_acc_ecg:.2f}%")


model_ecg.load_state_dict(torch.load('heart_ecg_model.pth'))
model_ecg.eval()

all_preds_ecg = []
all_labels_ecg = []
test_loss_ecg = 0.0

with torch.no_grad():
    for images, labels in test_ecg_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model_ecg(images)
        loss = criterion(outputs, labels)
        test_loss_ecg += loss.item()
        
        _, predicted = outputs.max(1)
        all_preds_ecg.extend(predicted.cpu().numpy())
        all_labels_ecg.extend(labels.cpu().numpy())

test_loss_ecg = test_loss_ecg / len(test_ecg_loader)
test_acc_ecg = accuracy_score(all_labels_ecg, all_preds_ecg)

print("ECG MODEL FINAL RESULTS")
print("="*50)
print(f"Train Accuracy: {train_accs_ecg[-1]:.2f}%")
print(f"Val Accuracy:   {val_accs_ecg[-1]:.2f}%")
print(f"Test Accuracy:  {test_acc_ecg*100:.2f}%")
print(f"\nOverfit gap (Train-Test): {(train_accs_ecg[-1]-test_acc_ecg*100):.2f}%")
print(f"\nTest Classification Report:\n{classification_report(all_labels_ecg, all_preds_ecg, target_names=['Normal', 'Heart Disease'])}")

cm = confusion_matrix(all_labels_ecg, all_preds_ecg)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title('ECG Model - Test Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

print("\nECG model saved as heart_ecg_model.pth")


print("="*80)
print("FINAL MODEL TRAINING SUMMARY")
print("="*80)

print("\n1. DIABETES PREDICTION")
print("-" * 50)
print("Tabular Model (XGBoost):")
print(f"  Train Accuracy: 81.30%")
print(f"  Val Accuracy:   79.87%")
print(f"  Test Accuracy:  75.97%")
print(f"  Overfit gap:    5.33%")
print(f"  Files: diabetes_xgboost_model.pkl, diabetes_scaler.pkl")

print("\nRetinal Image Model (EfficientNet-B0):")
print(f"  Train Accuracy: 93.49%")
print(f"  Val Accuracy:   95.09%")
print(f"  Test Accuracy:  94.41%")
print(f"  Overfit gap:    -0.92%")
print(f"  Files: diabetes_retinal_model.pth")

print("\n2. HEART DISEASE PREDICTION")
print("-" * 50)
print("Tabular Model (XGBoost):")
print(f"  Train Accuracy: 88.13%")
print(f"  Val Accuracy:   82.93%")
print(f"  Test Accuracy:  86.34%")
print(f"  Overfit gap:    1.79%")
print(f"  Files: heart_xgboost_model.pkl, heart_scaler.pkl")

print("\nECG Image Model (ResNet50):")
print(f"  Train Accuracy: 88.13%")
print(f"  Val Accuracy:   93.01%")
print(f"  Test Accuracy:  89.78%")
print(f"  Overfit gap:    -1.66%")
print(f"  Files: heart_ecg_model.pth")

print("\n" + "="*80)

print("\nFiles ready for download:")
files_list = [
    'diabetes_xgboost_model.pkl',
    'diabetes_scaler.pkl',
    'diabetes_retinal_model.pth',
    'heart_xgboost_model.pkl',
    'heart_scaler.pkl',
    'heart_ecg_model.pth'
]
for i, file in enumerate(files_list, 1):
    print(f"{i}. {file}")


get_ipython().getoutput("zip -r model.zip /kaggle/working")


from IPython.display import FileLink
FileLink(r'model.zip')
